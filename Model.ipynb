{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DohdTzpdYv1"
      },
      "source": [
        "## Étape 1 : Montage de Google Drive et Extraction du Dataset\n",
        "\n",
        "### 1.1 Montage de Google Drive\n",
        "La première étape consiste à monter Notre Google Drive dans Google Colab pour pouvoir accéder à vos fichiers stockés. Ce code utilise le module `google.colab.drive` pour monter votre Drive dans l'environnement Colab.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpjYlEAVe9Or"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip \"/content/drive/MyDrive/archive (8).zip\" -d \"/content/dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "offAxdrgeF7k"
      },
      "source": [
        "## Étape 2 : Chargement des Datasets d'Entraînement et de Validation\n",
        "\n",
        "### 2.1 Définition des Paramètres\n",
        "Avant de charger les datasets, nous définissons les paramètres nécessaires tels que les chemins vers les répertoires des données d'entraînement et de validation, la taille des images, ainsi que la taille des lots (batch size) pour l'entraînement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0cGHVTLlZDo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "training_path = \"/content/dataset/real_vs_fake/real-vs-fake/train\"\n",
        "validation_path = \"/content/dataset/real_vs_fake/real-vs-fake/valid\"\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    training_path,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    image_size=img_size,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    validation_path,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    image_size=img_size,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"sks\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNX-i4ZSeUoc"
      },
      "source": [
        "## Étape 3 : Prétraitement et Augmentation des Images\n",
        "\n",
        "### 3.1 Définition de la Fonction de Prétraitement\n",
        "Pour améliorer la généralisation du modèle, nous appliquons des transformations d'augmentation d'images. Ces transformations permettent d'introduire de la variabilité dans les données d'entraînement et d'éviter le sur-apprentissage (overfitting). Les transformations appliquées sont :\n",
        "\n",
        "- **Flip horizontal aléatoire** : Permet de retourner l'image de manière aléatoire sur l'axe horizontal.\n",
        "- **Contraste aléatoire** : Modifie le contraste de l'image dans une plage définie.\n",
        "- **Luminosité aléatoire** : Modifie la luminosité de l'image de manière aléatoire.\n",
        "- **Normalisation** : Convertit l'image en valeurs entre 0 et 1 en divisant par 255.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBYi7Irzd7tB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_image(image, label):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
        "    image = tf.image.random_brightness(image, 0.1)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "train_dataset = train_dataset.map(preprocess_image)\n",
        "validation_dataset = validation_dataset.map(preprocess_image)\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "validation_dataset = validation_dataset.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGuqWcEFe0UH"
      },
      "source": [
        "## Étape 4 : Exploration des Données (EDA)\n",
        "\n",
        "### 4.1 Visualisation d'un Lot d'Images\n",
        "Avant de commencer l'entraînement du modèle, il est important de visualiser les données pour vérifier leur qualité et leur distribution. Ici, nous récupérons un lot d'images et de labels à partir du dataset d'entraînement et les affichons à l'aide de `matplotlib`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71E1TIiAeZJF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "image_batch, label_batch = next(iter(train_dataset))\n",
        "\n",
        "\n",
        "def show_batch(image_batch, label_batch):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for n in range(min(25, len(image_batch))):\n",
        "        ax = plt.subplot(5, 5, n + 1)\n",
        "        plt.imshow(image_batch[n])\n",
        "        plt.title(int(label_batch[n]))\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "show_batch(image_batch.numpy(), label_batch.numpy())\n",
        "\n",
        "class_names = train_dataset.class_names\n",
        "train_labels = np.concatenate([y for x, y in train_dataset], axis=0)\n",
        "validation_labels = np.concatenate([y for x, y in validation_dataset], axis=0)\n",
        "\n",
        "\n",
        "def plot_class_distribution(labels, dataset_type):\n",
        "  unique, counts = np.unique(labels, return_counts=True)\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.bar(class_names, counts)\n",
        "  plt.title(f'Class Distribution for {dataset_type} Dataset')\n",
        "  plt.xlabel('Class Labels')\n",
        "  plt.ylabel('Number of Images')\n",
        "  plt.show()\n",
        "plot_class_distribution(train_labels, 'Training')\n",
        "plot_class_distribution(validation_labels, 'Validation')\n",
        "\n",
        "for images, labels in train_dataset.take(1):\n",
        "  print(\"Image shape:\", images.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUMcCBSnfTRu"
      },
      "source": [
        "## Étape 5 : Création du Modèle avec Fine-Tuning\n",
        "\n",
        "### 5.1 Importation du Modèle de Base (ResNet50)\n",
        "Dans cette étape, nous utilisons un modèle pré-entraîné de ResNet50 comme base. Ce modèle a été formé sur ImageNet et est utilisé pour extraire des caractéristiques d'images. Nous chargeons le modèle sans sa couche finale (`include_top=False`) pour pouvoir y ajouter nos propres couches de classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JHenP3V3FVIg"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,049,088\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,770,433</span> (94.49 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,770,433\u001b[0m (94.49 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,614,593</span> (93.90 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m24,614,593\u001b[0m (93.90 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">155,840</span> (608.75 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m155,840\u001b[0m (608.75 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "\n",
        "base_model = tf.keras.applications.ResNet50(\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False\n",
        ")\n",
        "base_model.trainable = True\n",
        "fine_tune_at = 20\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation=\"sigmoid\")  # Binary classification\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(learning_rate=0.0001),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt_UFgSVf-od"
      },
      "source": [
        "## Étape 6 : Entraînement du Modèle\n",
        "\n",
        "### 6.1 Initialisation de l'EarlyStopping\n",
        "Dans cette étape, nous utilisons la technique **EarlyStopping** pour arrêter l'entraînement du modèle lorsque la performance sur l'ensemble de validation cesse de s'améliorer. Cela permet d'éviter le sur-apprentissage et de sauvegarder le meilleur modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_ytWAnywCQK"
      },
      "outputs": [],
      "source": [
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCenUAcAgvQG"
      },
      "source": [
        "## Étape 7 : Préparation du Jeu de Données de Test\n",
        "\n",
        "### 7.1 Chargement du Jeu de Données de Test\n",
        "Dans cette étape, nous chargeons le jeu de données de test à partir du répertoire spécifié. Le jeu de données de test est utilisé pour évaluer la performance finale du modèle après l'entraînement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZC-tHP2fBE1"
      },
      "outputs": [],
      "source": [
        "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    '/content/dataset/real_vs_fake/real-vs-fake/test',\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_dataset = test_dataset.map(preprocess_image)\n",
        "\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK8GXX99g2bh"
      },
      "source": [
        "## Étape 8 : Évaluation du Modèle sur le Jeu de Données de Test\n",
        "\n",
        "### 8.1 Évaluation du Modèle\n",
        "Une fois que le modèle a été entraîné, nous procédons à son évaluation sur le jeu de données de test. Cela nous permet de mesurer la performance finale du modèle en termes de perte (loss) et de précision (accuracy).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dAGUcmXwIDI"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snhgm_xag_hq"
      },
      "source": [
        "## Étape 9 : Sauvegarde du Modèle Entraîné\n",
        "\n",
        "### 9.1 Sauvegarder le Modèle\n",
        "Une fois que le modèle a été entraîné et évalué, il est essentiel de le sauvegarder pour pouvoir l'utiliser ultérieurement sans avoir à le réentraîner. La méthode **model.save()** permet de sauvegarder le modèle dans un fichier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnXNqTAmbh_r"
      },
      "outputs": [],
      "source": [
        "model.save('mon_modele.keras')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
